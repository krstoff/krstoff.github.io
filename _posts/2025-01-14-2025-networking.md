---
layout: post
title:  "Writing an Orchestrator [2] - Networking"
---
[[part 1](/2025/01/14/part-01-agent.html)]

* TOC
{:toc}

## The Big Picture
We can run containers. Now we want to connect them to the Internet, and each other. 

Every pod receives an IPv6 address. This doesn't mean that a pod can't have more than one interface, like [bonded network cards for higher throughput](https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/8/html/configuring_and_managing_networking/configuring-network-bonding_configuring-and-managing-networking), but every pod needs at least *one* interface in order to facilitate the basic cluster functions.

Cluster networking will be particular to each situation, which is why Kubernetes and containerd make this pluggable via the Container Network Interface ([CNI](https://github.com/containernetworking/cni)). CNI plugins are just single-binaries that take arbitrary arguments and know how to ADD and DELETE network interfaces. These can be as simple as the default ones like Bridge or as complicated as [Calico](https://docs.tigera.io/calico/latest/about/). Ultimately what matters is that you have a way to pick IPs out of your assigned address space and can push routes to the networking infrastructure.

Our whole cluster will live in two /64 subnets on a single AWS VPC (why two? I'll explain later). If we stick to using globally addressable IPs then, assuming ingress is allowed, multi-VPC clusters will Just Work via the default outbound rules. No NAT? No problem. That's the beauty of IPv6. 

AWS recently launched the [Graviton](https://aws.amazon.com/ec2/graviton/getting-started/) family of EC2 instances that are custom designed for cloud workloads. The specific feature we want that only Graviton supports is attaching an entire /96 cidr block to a virtual network interface. Packets entering the VPC will automatically be routed to the correct network interface if its associated subnet exists. We don't have to do any routing work at all at the VPC level.

Every node brought up in our cluster will receive two interfaces: one with an automatically assigned public ip address, and the other with an entire /96 cidr block for container traffic specifically. This is the reason why we needed two subnets: on AWS, you can't use a subnet that automatically assigns public IP addresses to assign a whole cidr block to a network interface. So one of our subnets will be for node traffic (called the node-subnet), and the other will be for container traffic (called the container-subnet), and each node will have one interface from each.

Once a node has an entire CIDR block to play with, containers can be networked fairly trivially. Every time a pod comes up, we bring up an [ipvlan](https://developers.redhat.com/blog/2018/10/22/introduction-to-linux-interfaces-for-virtual-networking#) interface, marry it to the containerâ€™s network namespace, assign it a random IP address from our CIDR block, and then push a route into the kernel that associates the IP with that interface. Then we store our allocated address. On teardown we destroy the ipvlan interface and IP reservation.

This is so simple that it can't not work. It's the simplest possible setup for our cluster. And now, for some code.

## Infrastructure
The file `network.tf` defines the basic networking infrastructure in our VPC. We need `assign_generated_ipv6_cidr_block = true` in our `aws_vpc` block to enable IPv6. Two subnets, `node-subnet` and `container-subnet` are defined. You need to hand-pick two /64s, so pick the obvious ones: `ipv6_cidr_block = cidrsubnet(aws_vpc.cluster-vpc.ipv6_cidr_block, 8, 0)` for the node subnet and `ipv6_cidr_block = cidrsubnet(aws_vpc.cluster-vpc.ipv6_cidr_block, 8, 1)` for the containers. This is the first and second /64 in our vpc, respectively (64 - 56 = 8). Lastly, push a default `aws_route` for `::/0` to the internet gateway - fresh VPCs don't connect to the rest of the internet by default.

EC2 requires a machine image to launch an instance, so first, a word about `Packer`. Packer is a tool that builds and uploads AMIs for us. The orchestrator being developed here is intimately tied with requirements for the node it's running on so the same repo that contains `hyphae` contains infrastructure scripts as well. When I build an AMI I tag it with `packer build --var "commit-id=$(git rev-parse --short HEAD)" .`, and then use the commit id to pull the right machine image in terraform. The packer script itself provisions a minimal base image with the software we want (so far, our agent, containerd, and by the end of this post, the plugin), and then makes sure that the containerd daemon is enabled. [EBS](https://aws.amazon.com/ebs/) uses differential storage so keeping a few images stored should't cost that much money, but just remember that it isn't free and you need to clean up your images as you go along.

`node.tf` provisions our data plane. The `aws_ami` data block selects the right AMI based on the commit id we pass to terraform. Remember to change the owners section to your owner id, which you can find from the AWS image you upload. `node-eni` defines (`count = var.node_count`) Elastic Network Interfaces out of the `node-subnet`. These are basically just the node's primary network interfaces. `container-eni` creates the same number of ENIs for our dataplane, but:
- `ipv6_prefix_count = 1` tells the provisioner that we want an entire range of IP addresses assigned to this interface. This basically assigns one /96.
- `source_dest_check = false` tells the interface to allow traffic in that doesn't match its IP address exactly (pod IP address won't be the same as that interface's IP address, even though they live on the same subnet). This lets our node act as a router for its own pods.

The `aws_instance` block ties it all together by referencing the correct AMI, and binding the two network interfaces to the instance:
```
resource "aws_instance" "node" {
  count = var.node_count
  ami           = data.aws_ami.hyphae-node-image.id
  instance_type = "t3.micro"
  key_name = "skeleton-key"
  
  network_interface {
    network_interface_id = aws_network_interface.node-eni[count.index].id
    device_index = 0
  }
  network_interface {
    network_interface_id = aws_network_interface.container-eni[count.index].id
    device_index = 1
  }
  depends_on = [ aws_network_interface.container-eni ]
}
```

T3 instances are the smallest general purpose Graviton family instances that we can use. The `depends_on` field is required to make sure our network interfaces are created before our instances themselves.