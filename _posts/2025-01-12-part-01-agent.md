---
layout: post
title:  "Writing an Orchestrator [1] - The Agent"
---

* TOC
{:toc}

[[part 0](/2025/01/03/part-00-preliminaries.html)]

You have fifty large virtual machines and want to run some containers on them. What's next? Where do we start?

Containers are executed by a container engine like Docker, which is responsible for pulling container images, setting up networking, and invoking one of the available [runtimes](https://www.cncf.io/blog/2019/07/15/demystifying-containers-part-ii-container-runtimes/). The parts of Docker that were adopted by the CNCF and can be run separately from the high-level developer utilities form [containerd](https://containerd.io/), which is what we’ll be using. Should we just expose `containerd`’s API on a node’s port and call it a day? `containerd` doesn’t do any authorization, so our workloads will be susceptible to traffic both inside and outside the cluster. Additionally, bugs in the control plane may cause the node to get conflicting directions. Another piece of software should stand between `containerd` and the rest of the cluster. We’ll refer to this as the `agent`. The `agent` will be a lightweight program sitting on each node that interacts with `containerd` ’s API until the node is running the jobs we want, reports back to the rest of the cluster, and handles basic monitoring and restarts. All control plane traffic goes through the `agent`. In Kubernetes, this component is called the `kubelet`.  The `agent` will receive its node-specific instructions from the `controller`, which we’ll introduce later.

Our `agent` will manipulate `containerd` via [CRI](https://github.com/kubernetes/cri-api/blob/master/pkg/apis/runtime/v1/api.proto), translating reconciliation steps into CRI requests. CRI is a GRPC runtime interface that Kubernetes has standardized for talking to container engines like containerd and CRI-O. The advantage of picking this is over containerd's v1 client interface is that CRI has more features such as obtaining stdio handles, supports the CNI standard, and makes our agent more general than containerd. The disadvantage is that it’s a bigger API surface to learn than containerd’s, and brings along with it some design decisions I would have preferred to postpone until later, such as the Pod abstraction.    

![agent architecture](/assets/agent_arch.png) 

## Setup
The `agent` is a single Rust binary. We’ll use the [k8s-cri](https://crates.io/crates/k8s-cri) crate to make requests to `containerd` . We’ll also use the tokio crate for its asynchronous runtime. The protobuf tooling is also needed for k8s-cri to generate API wrappers.

```bash
mkdir agent; cd agent
sudo apt install protobuf-compiler
cargo init --bin
```

Make sure `containerd` is started and running.

```bash
systemctl enable containerd
systemctl start containerd
```

And lastly, you might find it helpful to [install crictl](https://github.com/kubernetes-sigs/cri-tools/releases), which is a tool for manipulating `containerd` from the command-line. I built up each operation in the `agent` incrementally, and used `crictl` to do clean-up or list the running containers, which helped verify that my code was correct.

All of the code discussed can be found [here](https://github.com/krstoff/hyphae/tree/6b5197333d076b98f576d2e3f51d14753db569f1/agent).

## Operations
There are two GRPC services under the scope of CRI: the image service, for managing images, and the runtime service, for creating and manipulating containers. These are exposed by equivalently named modules in the k8s_cri crate. [runtime.rs](https://github.com/krstoff/hyphae/blob/6b5197333d076b98f576d2e3f51d14753db569f1/agent/src/runtime.rs) is a thin layer over the service APIs that maps our domain types into CRI operations. `RuntimeClient::connect` connects to containerd over a socket at `run/containerd/containerd.sock` and proxies requests on behalf of the application.

The CRI API is well documented and easy to read in a sitting but I still wasn't clear on what was the minimum set of fields needed for a successful request so I tried filling in each request with `...Default::default()` and adding parameters until I got it to do what I wanted. I actually ended up crashing `containerd` a few times on unexpected nil errors - oops!

Under CRI, one creates a Sandbox first, and then adds containers to that sandbox one at a time. A Sandbox is a logical grouping of operating system resources that are shared by every container within it, such as CPU, memory, the network namespace, shared filesystem mounts - aka, a Pod in Kubernetes. The struct `runtime::SandboxConfig` has a method that maps hyphae sandbox configs into something that k8s_cri will take directly. Of particular interest is this part, in the linux options:

```
security_context: Some(cri::LinuxSandboxSecurityContext {
    namespace_options: Some(cri::NamespaceOption {
        network: cri::NamespaceMode::Node.into(),
        ...
    }),
    ...
}),
```

This option tells `containerd` to run our containers in the same networking namespace as the host. We have to do this for now because we haven’t assigned IP addresses to our pods. (Networking will be the next installation in this series.) Successfully creating a sandbox gives you an id for a sandbox in the ready state.

The next step before you can run a container in a sandbox is you need an identifier for an image stored in containerd. Be forewarned: calling `ImageServce::pull_image`will ALWAYS reach out and pull the digest for the image reference, instead of caching by default, because (1) names like "nginx:latest" are mutable, and (2) CRI has to support various pull policies (i.e. IfNotPresent, Always, and Never). The agent calls `ImageService::image_status` first and only calls `pull_image` when the requested image isn't present. The agent throttles the number of concurrent image pull requests—under sufficient load, TLS handshakes will start timing out in containerd. 

`RuntimeClient::create_container` takes a hyphae container config and creates a container from it, returning a container id upon success, pulling the image if necessary. `start_container` actually starts it, which is a separate operation from creation because the runtime needs a chance to call hooks that do additional modifications to the container. `stop_container` stops it, and `remove_container` removes it from the sandbox. Once every container has been stopped in a sandbox, the API permits you to destroy it (and never before).

## Events
Historically, the kubelet has just polled CRI periodically, but containerd provides an event stream you can subscribe to which limits the processing that the kubelet has to do. We'll rely on events as the primary source of container state and poll very infrequently (say, every fifteen seconds) just to catch dropped events. Most importantly we will not use the results of CRI operations to change observed state in order to avoid the use of shared state across concurrent tasks.

Container events and states are different. Containers can be in the Created, Running, Exited, or Unknown state (more on the last one, later). Create, Start, Stop, and Remove events are exactly what you would expect. 

![alt text](/assets/container_states.png)

`k8s_cri::v1::ContainerEventResponse` contains the container id, the event detected, the status of the pod it's in, and, surprisingly, the status of every single container that is running in the same pod. This is handy because when the agent is updating state based on observed events it can just replace the pod-state wholesale, and improves the agent's robustness against dropped lifecycle events.

The lifecycle event system is relatively new and has some unexpected behavior. I had to do quite a bit of experimenting to figure out what events are emitted and when. The design doesn’t really feel fully baked which is probably why it’s still disabled by default in the kubelet.
- There is no separate stream for pod lifecycle events. Almost every event carries its pod id with it, so you have to check that against the event id to see if it’s a pod event
- But pod deletion events are the only events that do not carry a pod id, only an event id. This makes sense because the id would be a dangling reference.
- You will always get a pod start event. You will infrequently get pod-create events, and it’s not clear when or why.
- If you stop a container, or delete it without calling stop first, you will always get a stop event.
- If you delete a container, you’ll receive a container deletion event, but if you delete a pod without deleting its containers, you’ll only get the pod deletion event, even though technically those containers were removed, too
- **If you delete every container from a pod before deleting the pod, you will rarely if ever receive a pod deletion event**

In order to ensure our state remains consistent, to terminate pods, the agent always stops every container in that pod first, without removing them, then deletes the pod. Container deletion will only be for mutating pods.

Updating the state based on observed events is implemented in the `State::observe` method in [state.rs](https://github.com/krstoff/hyphae/blob/6b5197333d076b98f576d2e3f51d14753db569f1/agent/src/state.rs#L29-L56).

## State
There are three different kinds of names here that interact, so try not to get confused. The first is *IDs*, for containers and pods. These are handles into actual objects in the `containerd` store. `containerd` does not know or care that it's being run as a cluster. It only runs what you tell it to run and gives you back handles for manipulating those things. The other two are *names* and *UUIDs*. *Names* are human readable strings that refer to cluster resources. We might have a pod named "app", for instance, with two constituent containers named "server" and "redis". Within a pod, two containers can't have the same name. In Kubernetes, two pods also can't have the same name. I am inclined not to care, but CRI will enforce the name conflict rule for us, so we're stuck with that. *UUIDs* are globally unique identifiers derived entirely from a deterministic hash of the spec for the resource (which does include its name). `containerd` enforces a UUID conflict rule as well (because it has no idea how we are deriving hashes). It is these UUIDs around which we can organize reconciliation.

`state::State` is just a mapping from UUIDs to pod IDs and the status of the pod, which itself is just a mapping from names to a container status. The `state::Target` we receive from the controller is the same, but from UUIDs to `PodConfig`. How do we know that the node is running the same set of pods as the set in the target? We simply have to check that our node is running a pod with that UUID. We don't have to check that the configuration changed, because if it did, the UUID would have, as well. The same goes for every (UUID, Container Name) pair. To check that the pod is running the containers we want, and only those, we just check to see that, for every name in the target pod's list of containers, we are running a container with that name on the node. 

## The Control Loop
The agent starts by polling the runtime for a list of currently running pods and containers on the node and initializing its state with it. Then, the control loop is event-triggered by any of the following:
- A new target is available
- A new list of events have arrived
- The agent has hit its state refresh interval, to catch any dropped lifecycle events. 15 seconds seems reasonable for this.

Then, the agent compares the observed state of the node against the target state. The relevant section is in the function [diff](https://github.com/krstoff/hyphae/blob/6b5197333d076b98f576d2e3f51d14753db569f1/agent/src/state.rs#L111-L197). There are four parts to this:
- Is every pod UID in target also in state?
- For every container name in a target pod, does a matching container exist in the state pod?
- For every pod that is running, should it be?
- For every pod that is meant to be running, is it only running the containers that it should be?

This produces a `Plan`, which is a tree of Steps that bring the node one step closer to the intended target, like adding a container, or removing an entire pod. Each step maps directly to the CRI operations explained earlier. When I was designing this part, I was making this much more complicated than it needed to be, because I was getting hung up on the dependencies of steps. It turns out you shouldn't concern yourself with what's happening in the future: all you need to worry about is, given whatever the state is right at the moment of observation, what is the *single next step* that you need to take to reconcile the state? This is because all of the future steps are a function of the present state and *only* the present state. All simple feedback controllers have always operated on this principle, even prior to computers.

Once a plan is created, you can create a `Worktree` from it. A worktree is just the result of transforming each `Step` into a (supervised) `Task` that attempts an operation until success. Building the worktree from a plan takes into consideration the previous worktree. If a generated step already has a counterpart in the previous worktree, we move it from the old tree into the new tree. The reason we need to keep track of tasks that are already running but still make and execute new plans is that some operations might be particularly long lived, such as downloading a very large image. **The rest of the worktree gets dropped**. This cancels all of the Tasks that were running but are no longer desired, and also cleans up the resources for the Tasks that completed. This is a situation where I'm quite thankful for RAII because move semantics results in a elegant solution that guarantees I don't have floating tasks still trying to change the node state. Worktrees are implemented in [worktree.rs](https://github.com/krstoff/hyphae/blob/6b5197333d076b98f576d2e3f51d14753db569f1/agent/src/worktree.rs). 

One gotcha I ran into is that under stress, containerd may suddenly stop knowing what state a container is actually in, resulting in the as of yet undiscussed container state `Unknown`. The correct step to generate from a container in state unknown is: nothing. Just wait for the state to normalize, or the next event to come in, or for a state-refresh interval tick and clarify the state of the pod. If a task related to that container is currently executing, do nothing except wait. 

From a few sections ago it should be obvious that the primary driver of control loop execution is the arrival of new events. Running the control loop on every single event is too much execution. Even coalescing the events together until the events queue is empty turned out to not be enough. I found that I had to debounce the events listener a bit - perhaps by two seconds - to make sure that all the events that wanted to arrive got there.

The only missing piece is where new Targets come from. We don't have a controller running yet so a stub process just sends a hard-coded target through a `tokio::sync::watch` variable. It would have been cooler at this stage to have it read from a file or something but this post is too long anyway. If you want a program that just watches a file then it is trivial to wire something up like that, but you might as well just use systemd.

## Results
In just under a thousand lines, this agent basically does everything we wanted it to do at this stage, which is: (1) run a list of pod configurations, (2) keep them running, and (3) make sure only those things are running. Compiled in release mode, the agent executable barely tops 4MB. On an eight-core laptop it spins up 100 pods with 3 alpine containers each in maybe ten seconds, and that's probably containerd maxing out - good enough for me. It spins down the same amount in a couple of seconds. Memory usage even with all the duplicate configurations and string copying never topped 8MB.

## Future Improvements
If `prost` (Rust's de facto Protobuf code generator) supported copy-on-write, reference-counted strings, that would make me very happy. This program does a lot of string copying and while testing it showed it didn't really matter, it would put my mind at ease.

Different kinds of resources may be added in the future that require a different kind of state machine logic. For instance, a `Pod` is supposed to have every `Container` stay in the `Running` state, but a Kubernetes `Job` actually desires for every `Container` to be in the `Exited` state, with return code 0. It'll beneficial in the future to add an abstraction over resources in general, but for now I found it desirable to keep everything as straight-line logic, which tends to be more readable.

This program could use a lot more concurrency control. For instance it doesn't have any concept of resource reservations yet, so it starts more containers before deleting containers to make room for e.g. RAM. Also, the amount of CPU consumed by the agent is unbounded. For nodes with a lot of pods that are just waiting around for requests it shouldn't matter too much but, for instance, a compute pool with nodes intended only for batch jobs is going to notice the agent going bananas for a few seconds.

The next step is implementing networking for pods.